{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'CardSort_AgeAdj', 'Flanker_AgeAdj', 'Gender', 'ListSort_AgeAdj', 'PMAT24_A_CR', 'PMAT24_A_RTCR', 'PMAT24_A_SI', 'PicSeq_AgeAdj', 'PicVocab_AgeAdj', 'ProcSpeed_AgeAdj', 'SCPT_TP', 'Subject', 'img_time_serie']\n",
      "Age (867, 1) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"84\": shape (1, 1), type \"|O\">\n",
      "CardSort_AgeAdj (867, 1) <HDF5 dataset \"5Jb\": shape (1, 1), type \"<f8\">\n",
      "Flanker_AgeAdj (867, 1) <HDF5 dataset \"4Xb\": shape (1, 1), type \"<f8\">\n",
      "Gender (867, 1) <HDF5 dataset \"0B\": shape (1, 1), type \"|O\">\n",
      "ListSort_AgeAdj (867, 1) <HDF5 dataset \"Wxd\": shape (1, 1), type \"<f8\">\n",
      "PMAT24_A_CR (867, 1) <HDF5 dataset \"3bc\": shape (1, 1), type \"<f8\">\n",
      "PMAT24_A_RTCR (867, 1) <HDF5 dataset \"1Dc\": shape (1, 1), type \"<f8\">\n",
      "PMAT24_A_SI (867, 1) <HDF5 dataset \"2pc\": shape (1, 1), type \"<f8\">\n",
      "PicSeq_AgeAdj (867, 1) <HDF5 dataset \"6vb\": shape (1, 1), type \"<f8\">\n",
      "PicVocab_AgeAdj (867, 1) <HDF5 dataset \"ZRc\": shape (1, 1), type \"<f8\">\n",
      "ProcSpeed_AgeAdj (867, 1) <HDF5 dataset \"Y6c\": shape (1, 1), type \"<f8\">\n",
      "SCPT_TP (867, 1) <HDF5 dataset \"Xjd\": shape (1, 1), type \"<f8\">\n",
      "Subject (867, 1) <HDF5 dataset \"b\": shape (1, 1), type \"<f8\">\n",
      "img_time_serie (867, 1) <HDF5 dataset \"ao\": shape (176, 268), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = '/home/mufan/mohan/gnn/dataset/2/HCPemotion_268_LR.mat'\n",
    "data_name = 'emotion_268_LR'\n",
    "\n",
    "# file_path = '/home/mufan/mohan/gnn/dataset/2/rest1_268_LR.mat'\n",
    "# data_name = 'rest1_268_LR'\n",
    "\n",
    "\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # print(list(file.keys()))  # View available variables\n",
    "    data_name = file[data_name]\n",
    "    # ['Age', 'CardSort_AgeAdj', 'Flanker_AgeAdj', 'Gender', 'ListSort_AgeAdj', 'PMAT24_A_CR', 'PMAT24_A_RTCR', 'PMAT24_A_SI', 'PicSeq_AgeAdj', 'PicVocab_AgeAdj', 'ProcSpeed_AgeAdj', 'SCPT_TP', 'Subject', 'img_time_serie']\n",
    "    print(list(data_name.keys()))\n",
    "    for key in data_name.keys():\n",
    "        print(key, data_name[key].shape, file[data_name[key][0][0]])\n",
    "        # break\n",
    "    \n",
    "    labels_dict = {}\n",
    "    for key in data_name.keys():\n",
    "        features = data_name[key]\n",
    "        sample_list = []\n",
    "        for sample in features:\n",
    "            sample = sample[0]\n",
    "            if key == 'Age':\n",
    "                sample = file[file[sample][0][0]][:]\n",
    "                sample = [item[0] for item in sample]\n",
    "            elif key == 'Gender':\n",
    "                sample = file[file[sample][0][0]][0][0]\n",
    "            elif key == 'img_time_serie':\n",
    "                sample = file[sample][:]\n",
    "            else:\n",
    "                sample = file[sample][0][0]\n",
    "            sample_list.append(sample)\n",
    "        labels_dict[key] = sample_list\n",
    "        \n",
    "    # with open(file_path.replace('.mat', '.pkl'), 'wb') as f:\n",
    "    #     pickle.dump(labels_dict, f)\n",
    "\n",
    "    # FC = {}\n",
    "    # for i in range(len(labels_dict['Subject'])):\n",
    "    #     key = str(int(labels_dict['Subject'][i]))\n",
    "    #     if key in ['109830', '236130', '614439','168139']:\n",
    "    #         continue\n",
    "        \n",
    "    #     time_series = labels_dict['img_time_serie'][i]\n",
    "\n",
    "    #     # 检查是否有常数时间序列\n",
    "    #     stddev = np.std(time_series, axis=0)  # 计算每行的标准差\n",
    "    #     if np.any(stddev == 0):  # 如果标准差为0，表示常数序列\n",
    "    #         constant_columns = np.where(stddev == 0)[0]  # 获取常数列的索引\n",
    "    #         print(f\"Subject {key} has constant time series in rows: {constant_columns}\")\n",
    "    #         continue\n",
    "        \n",
    "    #     stddev = np.std(time_series, axis=1)  # 计算每列的标准差\n",
    "    #     if np.any(stddev == 0):  # 如果标准差为0，表示常数序列\n",
    "    #         constant_columns = np.where(stddev == 0)[0]  # 获取常数列的索引\n",
    "    #         print(f\"Subject {key} has constant time series in columns: {constant_columns}\")\n",
    "    #         continue\n",
    "    \n",
    "    #     if np.any(np.isnan(time_series)):\n",
    "    #         nan_indices = np.argwhere(np.isnan(time_series))  # 获取NaN的索引\n",
    "    #         print(f\"Subject {key} has missing values at indices: {nan_indices}\")\n",
    "    \n",
    "    #     corr_matrix = np.corrcoef(time_series, rowvar=False)\n",
    "    #     corr_matrix = (corr_matrix + corr_matrix.T) / 2  # 对称化\n",
    "    #     FC[key] = corr_matrix\n",
    "    \n",
    "    # save_name = file_path.split('/')[-1].split('_')[0]\n",
    "    # save_path = os.path.join(os.path.dirname(file_path), 'FC_' + save_name + '.pkl')\n",
    "    # with open(save_path, 'wb') as f:\n",
    "    #     pickle.dump(FC, f)\n",
    "    \n",
    "    \n",
    "    label = {}\n",
    "    for i in range(len(labels_dict['Subject'])):\n",
    "        key = str(int(labels_dict['Subject'][i]))\n",
    "        if key in ['109830', '236130', '614439','168139']:\n",
    "            continue\n",
    "        label[key] = {}\n",
    "        for k in labels_dict.keys():\n",
    "            if k != 'Subject' and k != 'img_time_serie':\n",
    "                label[key][k] = labels_dict[k][i]\n",
    "    save_path = os.path.join(os.path.dirname(file_path), 'demo.pkl')\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/home/mufan/mohan/gnn/dataset/2/HCPemotion_268_LR.pkl'\n",
    "# data_path = '/home/mufan/mohan/gnn/dataset/2/rest1_268_LR.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    labels_dict = pickle.load(f)\n",
    "    \n",
    "# int: [\"Subject\", \"Gender\", \"PMAT24_A_CR\", \"PMAT24_A_SI\", \"SCPT_TP\"]\n",
    "# predict: ['PMAT24_A_CR', 'PMAT24_A_RTCR', 'PMAT24_A_SI', 'PicSeq_AgeAdj', 'PicVocab_AgeAdj', 'ProcSpeed_AgeAdj']\n",
    "df = pd.DataFrame(labels_dict)\n",
    "new_column_order = ['Subject', 'Gender', 'Age'] + [col for col in df.columns if col not in ['Subject', 'Gender', 'Age']]\n",
    "df = df[new_column_order]\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws下载文件\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "subjects = [int(subject) for subject in df['Subject'].values.tolist()]\n",
    "lh_data_path = ['stats/lh.aparc.stats', 'surf/lh.pial', 'surf/lh.white', \n",
    "               'label/lh.cortex.label', 'surf/lh.thickness', 'surf/lh.sphere.reg']  # 修正开头的斜杠\n",
    "rh_data_path = ['stats/rh.aparc.stats', 'surf/rh.pial', 'surf/rh.white',\n",
    "               'label/rh.cortex.label', 'surf/rh.thickness', 'surf/rh.sphere.reg']\n",
    "data_path = lh_data_path + rh_data_path + ['mri/brain.mgz']\n",
    "\n",
    "\n",
    "def download_file(subject, path):\n",
    "    \"\"\"单个文件的下载任务\"\"\"\n",
    "    subject = str(subject)  # 确保subject转换为字符串\n",
    "    src = f\"s3://hcp-openaccess/HCP_1200/{subject}/T1w/{subject}/{path.lstrip('/')}\"  # 处理路径斜杠\n",
    "    dest_dir = os.path.join(\"/home/mufan/mohan/gnn/dataset/2/subjects\", subject, os.path.dirname(path))\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    dst = os.path.join(dest_dir, os.path.basename(path))\n",
    "    \n",
    "    # 使用更安全的命令格式\n",
    "    cmd = [\"aws\", \"s3\", \"cp\", src, dst]\n",
    "    result = subprocess.run(cmd, check=True, stderr=subprocess.PIPE, text=True)\n",
    "    return result\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    futures = [executor.submit(download_file, subject, path) for subject in subjects for path in data_path]\n",
    "    for future in tqdm(futures):\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('/home/mufan/mohan/gnn/results.json') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# 存储结果的字典\n",
    "results = {}\n",
    "\n",
    "# 遍历数据集名\n",
    "for dataset_name, models in dataset.items():\n",
    "    # 为每个数据集创建一个模型的字典\n",
    "    dataset_results = {}\n",
    "    for model_name, runs in models.items():\n",
    "        # 获取所有随机数下的 RMSE\n",
    "        rmse_values = [run['best_test_rmse'] for run in runs.values()]\n",
    "        # 计算均值和标准差\n",
    "        mean_rmse = np.mean(rmse_values)\n",
    "        std_rmse = np.std(rmse_values)\n",
    "        # 保存均值和标准差\n",
    "        dataset_results[model_name] = (mean_rmse, std_rmse)\n",
    "    # 将该数据集的模型结果保存到结果字典中\n",
    "    results[dataset_name] = dataset_results\n",
    "\n",
    "# 将结果转换为 DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "third_dataset = df.columns[2]  # 获取第三个数据集的列名\n",
    "sorted_df = df.sort_values(\n",
    "    by=third_dataset,\n",
    "    key=lambda col: col.apply(lambda x: x[0])\n",
    ")\n",
    "df_formatted = sorted_df.applymap(lambda x: f\"{x[0]:.2f} ± {x[1]:.2f}\")\n",
    "df_formatted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import Explainer, GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载NIfTI文件\n",
    "img_path = '/home/mufan/mohan/gnn/dataset/2/parcellation/shen_2mm_268_parcellation.nii.gz'\n",
    "img = nib.load(img_path)\n",
    "\n",
    "# 可视化设置\n",
    "# 使用 nilearn 的 plot_roi 显示分区图谱（离散标签）\n",
    "plotting.plot_roi(\n",
    "    img,\n",
    "    title=\"Shen 268 Parcellation\",\n",
    "    cmap='Paired',        # 适合离散标签的颜色映射\n",
    "    display_mode='ortho',  # 正交三视图\n",
    "    draw_cross=False,      # 是否显示定位十字\n",
    "    colorbar=True          # 显示颜色条\n",
    ")\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "# 构建第二个数据集的T1\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('/home/mufan/mohan/gnn/dataset/2/parcellation/label.csv')\n",
    "lobes_count = {}\n",
    "result = []\n",
    "for idx, row in df.iterrows():\n",
    "    lobe = row['Lobes']\n",
    "    if lobe not in lobes_count:\n",
    "        lobes_count[lobe] = 0\n",
    "    lobes_count[lobe] += 1\n",
    "    result.append(f\"{lobe}_{lobes_count[lobe]}\")\n",
    "print(len(result))\n",
    "\n",
    "def get_data_dict(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data_dict = {}\n",
    "    current_structure = {}\n",
    "    keys = ['NumVert', 'SurfArea', 'GrayVol', 'ThickAvg', 'ThickStd', 'MeanCurv', 'GausCurv', 'FoldInd', 'CurvInd']\n",
    "    for line in lines:\n",
    "        if \"structure is\" in line:\n",
    "            if current_structure:\n",
    "                data_dict[current_structure['StructName']] = {\n",
    "                    key: current_structure[key] for key in keys\n",
    "                }\n",
    "            current_structure = {'StructName': int(line.strip().split()[-1].replace('\"', '').replace('cluster', ''))}\n",
    "        \n",
    "        elif \"number of vertices\" in line:\n",
    "            current_structure['NumVert'] = re.search(r\"(\\d+)\", line).group(1)\n",
    "        elif \"total surface area\" in line:\n",
    "            current_structure['SurfArea'] = re.search(r\"(\\d+)\", line).group(1)\n",
    "        elif \"total gray matter volume\" in line:\n",
    "            current_structure['GrayVol'] = re.search(r\"(\\d+)\", line).group(1)\n",
    "        elif \"average cortical thickness\" in line:\n",
    "            values = re.findall(r\"([\\d\\.]+)\", line)\n",
    "            current_structure['ThickAvg'] = values[0]\n",
    "            current_structure['ThickStd'] = values[1]\n",
    "        elif \"average integrated rectified mean curvature\" in line:\n",
    "            current_structure['MeanCurv'] = re.search(r\"([\\d\\.]+)\", line).group(1)\n",
    "        elif \"average integrated rectified Gaussian curvature\" in line:\n",
    "            current_structure['GausCurv'] = re.search(r\"([\\d\\.]+)\", line).group(1)\n",
    "        elif \"folding index\" in line:\n",
    "            current_structure['FoldInd'] = re.search(r\"(\\d+)\", line).group(1)\n",
    "        elif \"intrinsic curvature index\" in line:\n",
    "            current_structure['CurvInd'] = re.search(r\"([\\d\\.]+)\", line).group(1)\n",
    "\n",
    "    # 最后一个结构的保存\n",
    "    if current_structure:\n",
    "        data_dict[current_structure['StructName']] = {\n",
    "            key: current_structure[key] for key in keys\n",
    "        }\n",
    "    return data_dict\n",
    "\n",
    "def get_df(id):\n",
    "    file_path = f'/home/mufan/mohan/gnn/dataset/2/subjects/{id}/stats/rh.aparc.shen268.stats'  # 替换为你自己的文件路径\n",
    "    rh = get_data_dict(file_path)\n",
    "    file_path = f'/home/mufan/mohan/gnn/dataset/2/subjects/{id}/stats/lh.aparc.shen268.stats'  # 替换为你自己的文件路径\n",
    "    lh = get_data_dict(file_path)\n",
    "    data = {}\n",
    "    for i in range(1, 269):\n",
    "        if i < 134 and i in rh:\n",
    "            data[i-1] = rh[i]\n",
    "        elif i >= 134 and i in lh:\n",
    "            data[i-1] = lh[i]\n",
    "        else:\n",
    "            data[i-1] = {\n",
    "                'NumVert': '0',\n",
    "                'SurfArea': '0',\n",
    "                'GrayVol': '0',\n",
    "                'ThickAvg': '0',\n",
    "                'ThickStd': '0',\n",
    "                'MeanCurv': '0',\n",
    "                'GausCurv': '0',\n",
    "                'FoldInd': '0',\n",
    "                'CurvInd':'0'\n",
    "            }\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(data)):\n",
    "        struct_name = result[i]\n",
    "        row = {'StructName': struct_name, **data[i]}\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "ids=\"'100206' '100307' '100408' '100610' '101006' '101107' '101309' '101915' '102008' '102109' '102311' '102513' '102614' '102715' '102816' '103111' '103212' '103414' '103515' '103818' '104012' '104416' '104820' '105014' '105115' '105216' '105620' '105923' '106016' '106319' '106521' '106824' '107018' '107321' '107422' '107725' '108020' '108121' '108222' '108323' '108525' '108828' '109123' '109325' '109830' '110007' '110411' '110613' '111009' '111211' '111312' '111413' '111514' '111716' '112112' '112314' '112516' '112920' '113215' '113316' '113619' '113922' '114217' '114318' '114419' '114621' '114823' '114924' '115017' '115219' '115320' '115724' '115825' '116221' '116524' '116726' '117021' '117122' '117324' '117930' '118023' '118124' '118225' '118528' '118730' '118831' '118932' '119025' '119126' '120111' '120212' '120414' '120515' '120717' '121416' '121618' '121921' '122317' '122418' '122620' '122822' '123117' '123420' '123521' '123723' '123824' '123925' '124220' '124422' '124624' '124826' '125222' '125424' '125525' '126325' '126426' '126628' '127226' '127327' '127630' '127731' '127832' '127933' '128026' '128127' '128632' '128935' '129028' '129129' '129331' '129634' '129937' '130013' '130114' '130316' '130417' '130518' '130619' '130720' '130821' '130922' '131217' '131419' '131722' '131823' '131924' '132017' '133019' '133625' '133827' '133928' '134021' '134223' '134324' '134425' '134728' '134829' '135124' '135225' '135528' '135629' '135730' '135932' '136227' '136631' '136732' '136833' '137027' '137128' '137229' '137532' '137633' '137936' '138130' '138231' '138332' '138534' '138837' '139233' '139435' '139637' '139839' '140117' '140319' '140824' '140925' '141119' '141422' '141826' '142828' '143224' '143325' '143830' '144125' '144428' '144731' '144832' '144933' '145127' '145632' '145834' '146129' '146331' '146432' '146533' '146735' '146836' '146937' '147030' '147636' '147737' '148032' '148133' '148335' '148436' '148941' '149236' '149337' '149539' '149741' '149842' '150625' '150726' '150928' '151223' '151324' '151425' '151526' '151627' '151728' '151829' '151930' '152225' '152427' '152831' '153025' '153126' '153227' '153429' '153631' '153833' '153934' '154229' '154330' '154431' '154532' '154734' '154835' '154936' '155635' '155938' '156031' '156233' '156334' '156435' '156536' '156637' '157336' '157437' '157942' '158035' '158136' '158338' '158540' '158843' '159138' '159239' '159340' '159441' '159744' '160123' '160729' '160830' '161327' '161630' '161731' '161832' '162026' '162228' '162329' '162733' '162935' '163129' '163331' '163432' '163836' '164030' '164131' '164636' '164939' '165032' '165436' '165638' '165840' '165941' '166438' '166640' '167036' '167238' '167440' '167743' '168139' '168240' '168341' '168745' '168947' '169040' '169444' '169545' '169949' '170631' '171330' '171532' '171633' '172029' '172130' '172332' '172433' '172534' '172938' '173334' '173435' '173536' '173637' '173738' '173839' '173940' '174437' '174841' '175035' '175136' '175237' '175338' '175439' '175742' '176037' '176239' '176441' '176542' '176845' '177140' '177241' '177645' '178142' '178243' '178647' '178748' '178849' '178950' '179245' '179346' '180129' '180230' '180432' '180533' '180735' '180836' '180937' '181131' '181232' '181636' '182436' '182739' '183034' '185038' '185139' '185341' '185442' '185846' '185947' '186040' '186141' '186444' '186545' '186848' '187143' '187345' '187547' '187850' '188145' '188347' '188448' '188549' '188751' '189349' '189450' '189652' '190031' '191033' '191235' '191336' '191841' '191942' '192035' '192136' '192237' '192439' '192540' '192641' '192843' '193239' '193845' '194140' '194443' '194645' '194746' '195041' '195445' '195849' '195950' '196144' '196346' '196750' '197348' '197550' '198249' '198350' '198451' '198653' '199150' '199352' '199453' '199655' '199958' '200008' '200109' '200311' '200513' '200614' '200917' '201111' '201414' '201515' '201818' '202113' '202719' '203418' '203923' '204319' '204420' '204521' '204622' '205119' '205220' '205725' '205826' '206222' '206323' '206525' '206727' '206828' '207123' '207426' '208024' '208125' '208226' '208327' '209127' '209228' '209329' '209834' '209935' '210011' '210112' '210415' '210617' '211114' '211215' '211316' '211417' '211619' '211720' '211821' '211922' '212015' '212116' '212217' '212318' '212419' '212823' '213017' '213522' '214019' '214221' '214423' '214726' '217126' '217429' '219231' '220721' '221319' '223929' '224022' '227432' '227533' '228434' '231928' '233326' '236130' '237334' '238033' '239136' '239944' '245333' '246133' '248339' '249947' '250427' '250932' '251833' '255639' '255740' '256540' '257542' '257845' '257946' '263436' '268749' '268850' '270332' '274542' '275645' '280739' '281135' '283543' '285345' '285446' '286347' '286650' '287248' '289555' '290136' '293748' '295146' '297655' '298051' '298455' '299154' '299760' '300618' '300719' '303119' '303624' '304020' '304727' '305830' '307127' '308129' '308331' '309636' '310621' '311320' '314225' '316633' '316835' '317332' '318637' '320826' '321323' '322224' '325129' '329440' '329844' '330324' '333330' '334635' '336841' '339847' '341834' '342129' '346137' '346945' '348545' '349244' '350330' '352132' '352738' '353740' '356948' '358144' '360030' '361234' '361941' '365343' '366042' '366446' '368753' '371843' '376247' '377451' '378756' '378857' '379657' '380036' '381038' '381543' '382242' '385046' '385450' '386250' '387959' '389357' '390645' '391748' '392447' '392750' '393247' '393550' '394956' '395251' '395756' '395958' '397154' '397760' '397861' '401422' '406432' '406836' '412528' '413934' '414229' '415837' '419239' '421226' '422632' '424939' '429040' '432332' '433839' '436239' '436845' '441939' '445543' '448347' '449753' '453441' '453542' '454140' '456346' '459453' '461743' '463040' '465852' '467351' '468050' '469961' '475855' '479762' '480141' '481042' '481951' '485757' '486759' '495255' '497865' '499566' '500222' '506234' '510225' '510326' '512835' '513130' '513736' '516742' '517239' '518746' '519647' '519950' '520228' '522434' '523032' '524135' '525541' '529549' '529953' '530635' '531536' '536647' '540436' '541640' '541943' '545345' '548250' '552241' '552544' '553344' '555348' '555651' '555954' '557857' '558657' '558960' '559053' '559457' '561242' '561444' '561949' '562345' '562446' '565452' '566454' '567052' '567759' '567961' '568963' '570243' '571144' '572045' '573249' '573451' '576255' '578057' '579665' '579867' '580044' '580650' '580751' '581349' '581450' '583858' '585256' '585862' '586460' '587664' '588565' '589567' '590047' '592455' '594156' '597869' '598568' '599065' '599469' '599671' '601127' '604537' '609143' '611938' '613538' '614439' '615744' '616645' '617748' '618952' '620434' '622236' '623844' '626648' '627549' '627852' '628248' '633847' '634748' '635245' '638049' '645450' '645551' '647858' '654350' '654552' '654754' '656253' '656657' '657659' '660951' '662551' '663755' '664757' '665254' '667056' '668361' '671855' '672756' '673455' '675661' '677766' '677968' '679568' '679770' '680250' '680452' '680957' '683256' '685058' '686969' '687163' '689470' '690152' '692964' '693764' '694362' '695768' '698168' '700634' '702133' '704238' '707749' '709551' '715041' '715647' '715950' '720337' '723141' '724446' '725751' '727553' '728454' '729557' '731140' '732243' '734045' '735148' '737960' '742549' '744553' '748258' '749058' '749361' '751348' '753150' '753251' '756055' '757764' '759869' '761957' '763557' '765056' '765864' '767464' '769064' '770352' '771354' '773257' '774663' '779370' '782561' '783462' '784565' '786569' '788674' '788876' '789373' '792564' '792766' '792867' '793465' '800941' '802844' '803240' '809252' '810843' '812746' '814548' '814649' '815247' '816653' '818455' '818859' '820745' '825048' '825553' '825654' '826353' '826454' '828862' '832651' '833148'\"\n",
    "ids=ids.split(' ')\n",
    "ids = [int(id.replace(\"'\", '')) for id in ids]\n",
    "\n",
    "data = {}\n",
    "for id in ids:\n",
    "    if id in ['109830', '236130', '614439','168139']:\n",
    "        print(f\"Subject {id} has missing values\")\n",
    "        continue\n",
    "    df = get_df(id)\n",
    "    data[str(id)] = df\n",
    "\n",
    "with open('/home/mufan/mohan/gnn/dataset/2/T1.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/home/mufan/mohan/gnn/dataset/2/demo.pkl', 'rb') as f:\n",
    "# with open('/home/mufan/mohan/gnn/dataset/2/FC_HCPemotion.pkl', 'rb') as f:\n",
    "    labels_dict = pickle.load(f)\n",
    "labels_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计miss knn结果\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "# 初始化模型名称\n",
    "model_names = ['NeuroPath', 'MHGCN', 'Mew']\n",
    "\n",
    "with open('/home/mufan/mohan/gnn/dataset_1_miss_results.json') as f:\n",
    "    results = json.load(f)\n",
    "    \n",
    "# 创建空字典存储最终的数据框\n",
    "final_dfs = {model_name: pd.DataFrame() for model_name in model_names}\n",
    "\n",
    "# 遍历每个 model_name\n",
    "for model_name in model_names:\n",
    "    # 初始化一个空的DataFrame\n",
    "    model_df = pd.DataFrame()\n",
    "    \n",
    "    # 遍历所有 target_label\n",
    "    for target_label in results:\n",
    "        # 创建一个列表，存储当前target_label的每一行设置数据\n",
    "        rows = []\n",
    "        \n",
    "        # 遍历 miss 和 gen_miss\n",
    "        for miss in results[target_label].get(model_name, {}):\n",
    "            for gen_miss in results[target_label][model_name][miss]:\n",
    "                # 遍历不同的seed\n",
    "                best_rmse_list = []\n",
    "                for seed in results[target_label][model_name][miss][gen_miss]:\n",
    "                    best_rmse_list.append(results[target_label][model_name][miss][gen_miss][seed]['best_test_rmse'])\n",
    "                \n",
    "                # 计算平均值和标准差\n",
    "                mean_rmse = np.mean(best_rmse_list)\n",
    "                std_rmse = np.std(best_rmse_list)\n",
    "                \n",
    "                # 格式化字符串\n",
    "                if miss == 'none':\n",
    "                    if gen_miss == 'pad':\n",
    "                        setting_name = f\"{model_name}_full_data\"\n",
    "                    else:\n",
    "                        setting_name = f\"{model_name}_drop_miss\"\n",
    "                else:\n",
    "                    setting_name = f\"{model_name}_miss{miss}_gen.{gen_miss}\"\n",
    "                result_str = f\"{mean_rmse:.3f}±{std_rmse:.3f}\"\n",
    "                \n",
    "                # 添加到行\n",
    "                rows.append((setting_name, result_str))\n",
    "        \n",
    "        # 将行数据转换为DataFrame，并设置columns\n",
    "        target_df = pd.DataFrame(rows, columns=['Setting', target_label])\n",
    "        \n",
    "        # 将当前的target_df合并到模型对应的model_df中\n",
    "        if model_df.empty:\n",
    "            model_df = target_df\n",
    "        else:\n",
    "            model_df = pd.merge(model_df, target_df, on='Setting', how='outer')\n",
    "\n",
    "    # 存储结果\n",
    "    final_dfs[model_name] = model_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Setting</th>\n",
       "      <th>nih_crycogcomp_ageadjusted</th>\n",
       "      <th>nih_totalcogcomp_ageadjusted</th>\n",
       "      <th>nih_fluidcogcomp_ageadjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MHGCN_drop_miss</td>\n",
       "      <td>16.885±1.258</td>\n",
       "      <td>16.354±1.140</td>\n",
       "      <td>17.435±1.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MHGCN_full_data</td>\n",
       "      <td>16.934±1.203</td>\n",
       "      <td>16.354±1.140</td>\n",
       "      <td>17.435±1.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MHGCN_missfc_gen.knn</td>\n",
       "      <td>17.160±1.294</td>\n",
       "      <td>16.377±1.084</td>\n",
       "      <td>17.385±1.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MHGCN_missfc_gen.pad</td>\n",
       "      <td>17.167±1.277</td>\n",
       "      <td>16.331±1.097</td>\n",
       "      <td>17.506±1.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MHGCN_misssc_gen.knn</td>\n",
       "      <td>16.946±1.179</td>\n",
       "      <td>16.292±1.183</td>\n",
       "      <td>17.422±1.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MHGCN_misssc_gen.pad</td>\n",
       "      <td>16.930±1.167</td>\n",
       "      <td>16.356±1.038</td>\n",
       "      <td>17.525±1.316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Setting nih_crycogcomp_ageadjusted  \\\n",
       "0       MHGCN_drop_miss               16.885±1.258   \n",
       "1       MHGCN_full_data               16.934±1.203   \n",
       "2  MHGCN_missfc_gen.knn               17.160±1.294   \n",
       "3  MHGCN_missfc_gen.pad               17.167±1.277   \n",
       "4  MHGCN_misssc_gen.knn               16.946±1.179   \n",
       "5  MHGCN_misssc_gen.pad               16.930±1.167   \n",
       "\n",
       "  nih_totalcogcomp_ageadjusted nih_fluidcogcomp_ageadjusted  \n",
       "0                 16.354±1.140                 17.435±1.238  \n",
       "1                 16.354±1.140                 17.435±1.239  \n",
       "2                 16.377±1.084                 17.385±1.196  \n",
       "3                 16.331±1.097                 17.506±1.198  \n",
       "4                 16.292±1.183                 17.422±1.267  \n",
       "5                 16.356±1.038                 17.525±1.316  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dfs['MHGCN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "path = '/home/mufan/mohan/gnn/dataset/2/FC_Fisher_Z_transformed.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.tensor(np.array(list(data.values())))\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = []\n",
    "for key in data.keys():\n",
    "    row = {'Subject': key}\n",
    "    for k, v in data[key].items():\n",
    "        row[k] = v\n",
    "    result.append(row)\n",
    "df = pd.DataFrame(result)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
